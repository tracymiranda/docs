---
title: 'Ollama'
description: 'Get Ollama up and running in Wave AI'
---

<Warning>
## Wave Terminal Documentation Has Moved

As of September 2024, this version of Wave Terminal is deprecated. To learn more about our new version (>=v0.8.0), check out [www.waveterm.dev](https://www.waveterm.dev). To find documentation for our new version, check out [docs.waveterm.dev](https://docs.waveterm.dev).
</Warning>

[Ollama](https://ollama.com/) is an open-source language model that offers a powerful and flexible alternative to proprietary LLMs, allowing you to run the model locally or on your own server infrastructure. Ollama provides high-quality language generation and understanding capabilities while giving you full control over your data and privacy.

<Tip>
To see a full list of supported LLM providers, please visit the [Third-Party LLM Support](/features/waveAI#third-party-llm-support-byollm) section in the Wave AI features page.
</Tip>

## Installation
Please visit Ollama's [GitHub](https://github.com/ollama/ollama?tab=readme-ov-file) page for instructions on downloading and installing Ollama, as well as a quickstart guide and a full list of supported models.

## Configuration
After installing and configuring Ollama, you can start using it in Wave by setting two parameters: `aibaseurl` and `aimodel`. These parameters can be set either through the UI or from the command line, but please note that the parameter names are slightly different depending on the method you choose.

### **Parameters**

- **AI Base URL:** Set this parameter to the base URL or endpoint that Wave AI should query. For Ollama running locally, use http://localhost:11434/v1. Please note that the port number `11434` may be different depending on your specific installation. For remote Ollama instances, replace `localhost` with the appropriate hostname or IP address of the server where Ollama is running. If the port number is different from the default `11434`, update it accordingly in the URL.
- **AI Model**: Specify the Ollama model you want to use. This can be any "pulled" model in Ollama and doesn't need to be actively running. To discover available models, use the `ollama list` command in your terminal.

### Configuring via the UI
To configure Ollama from Wave's user interface, navigate to the "Settings" menu and set the `AI Base URL` and `AI Model` parameters as described in the previous section.

### Configuring via the CLI
To configure Ollama using the command line, set the `aibaseurl` and `aimodel` parameters using the [/client:set](/reference/slashcommands#client-set) command, as shown in the example below.

```
/client:set aibaseurl=<your-ollama-base-url>
/client:set aimodel=<your-ollama-model-name>
```

## Usage
Once you have installed and configured Ollama, you can start using it in Wave. There are two primary ways to interact with your newly configured LLM: [Interactive Mode](/features/waveAI#interactive) and by using the [/chat](/features/waveAI#chat-command) command.

- **Interactive Mode:** To enter Interactive Mode, click the "Wave AI" button in the command box or use the `ctrl + space` shortcut. This will open an interactive chat session where you can have a continuous conversation with the AI assistant powered by your Ollama model.
- **/chat:** Alternatively, you can use the [/chat](/features/waveAI#chat-command) command followed by your question to get a quick answer from your Ollama model directly in the terminal.

## Troubleshooting
If you encounter issues while using Ollama with Wave AI, consider the following troubleshooting steps:

- **Connection failures:** If Wave AI fails to connect to Ollama or returns an error message, [verify that Ollama is running](https://github.com/ollama/ollama?tab=readme-ov-file#rest-api) and accessible from the system where Wave is installed. Check the Ollama logs for any error messages or indications of why the connection might be failing.
- **Timeouts:** If you're unable to complete a query or incur frequent timeouts, try adjusting the `aitimeout` parameter to a higher value. This will give Ollama more time to process and respond to your requests, especially if you are running it on a system with limited hardware resources.
- **Incorrect base URL or port:** Ensure that the `aibaseurl` parameter points to the correct URL and port number where Ollama is running. If you have changed the default port or are running Ollama on a remote server, update the URL accordingly.
- **Incorrect model selection:** If you have multiple Ollama models installed, make sure to set the `aimodel` parameter to the specific model you want to use. You can list available models using the ollama list command in your terminal.
- **Unexpected behavior or inconsistent results:** If you encounter unexpected behavior or inconsistent results when using Ollama with Wave AI, try [resetting](#reset-wave-ai) the `aibaseurl` and `aimodel` parameters to their default values and reconfiguring Ollama from scratch. This can help rule out any configuration issues that might be causing problems.

<Tip>
If you continue to face issues after trying these troubleshooting steps, please see the [Additional Resources](#additional-resources) section below for further assistance, or feel free to reach out to us on [Discord](https://discord.gg/XfvZ334gwU).
</Tip>

## Reset Wave AI
At any time if you find that you wish to return to the default Wave AI experience, you can reset the `aibaseurl` and `aimodel` parameters to their default state by using the following commands.

```
/client:set aibaseurl=
/client:set aimodel=
```
**Note:** This can also be done in the UI just as described in previous steps.

## Additional Resources
* [Ollama Website](https://ollama.com/)
* [Ollama GitHub Page](https://github.com/ollama/ollama?tab=readme-ov-file)
* [Ollama Support (Discord)](https://discord.com/invite/ollama)